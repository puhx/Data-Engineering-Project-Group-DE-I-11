{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieving the data to increase in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current size of the dataset (10000, 3)\n",
      "       bpm  loudness  year\n",
      "0   92.198   -11.197     0\n",
      "1  121.274    -9.843  1969\n",
      "2  100.070    -9.689     0\n",
      "3  119.293    -9.013  1982\n",
      "4  129.738    -4.501  2007\n",
      "current size of the dataset 0.26702880859375 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_hdf('../data/aggregated_song_features.h5', key='data')\n",
    "print('current size of the dataset', df.shape)\n",
    "print(df.head())\n",
    "# calculate the size of the dataset, in bytes\n",
    "print('current size of the dataset', df.memory_usage(index=True).sum()/(1024**2), 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the doubled dataset (20000, 3)\n",
      "size of the doubled dataset 0.5340576171875 MB\n",
      "size of the quadrupled dataset (40000, 3)\n",
      "size of the quadrupled dataset 1.068115234375 MB\n",
      "size of the 10x dataset (100000, 3)\n",
      "size of the 10x dataset 2.6702880859375 MB\n"
     ]
    }
   ],
   "source": [
    "#double size\n",
    "df_double = pd.concat([df +1, df +2])\n",
    "df_double['year'] = df_double['year'].replace(1, 0)\n",
    "df_double['year'] = df_double['year'].replace(2, 0)\n",
    "\n",
    "print('size of the doubled dataset', df_double.shape)\n",
    "print('size of the doubled dataset', df_double.memory_usage(index=True).sum()/(1024**2), 'MB')\n",
    "\n",
    "# save the dataset to a new file\n",
    "df_double.to_hdf('../data/aggregated_song_features_double.h5', key='data', mode='w')\n",
    "\n",
    "# quadruple size\n",
    "df_quadruple = pd.concat([df_double +3, df_double +4])\n",
    "df_quadruple['year'] = df_quadruple['year'].replace(3, 0)\n",
    "df_quadruple['year'] = df_quadruple['year'].replace(4, 0)\n",
    "\n",
    "print('size of the quadrupled dataset', df_quadruple.shape)\n",
    "print('size of the quadrupled dataset', df_quadruple.memory_usage(index=True).sum()/(1024**2), 'MB')\n",
    "\n",
    "df_quadruple.to_hdf('../data/aggregated_song_features_quadruple.h5', key='data', mode='w')\n",
    "\n",
    "\n",
    "# 10 times the size\n",
    "df_10 = pd.concat([df_double +5, df_double +6, df_double +7, df_double +8, df_double +9])\n",
    "df_10['year'] = df_10['year'].replace(5, 0)\n",
    "df_10['year'] = df_10['year'].replace(6, 0)\n",
    "df_10['year'] = df_10['year'].replace(7, 0)\n",
    "df_10['year'] = df_10['year'].replace(8, 0)\n",
    "df_10['year'] = df_10['year'].replace(9, 0)\n",
    "\n",
    "print('size of the 10x dataset', df_10.shape)\n",
    "print('size of the 10x dataset', df_10.memory_usage(index=True).sum()/(1024**2), 'MB')\n",
    "\n",
    "df_10.to_hdf('../data/aggregated_song_features_10x.h5', key='data', mode='w')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
