{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f86351-cca2-4a7e-a666-719608545f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    round, count, avg, col, udf, abs, mean, expr\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# Other imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba58cf60-45f5-4bb5-bcd2-aee90854ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/10 22:13:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/10 22:13:09 WARN StandaloneSchedulerBackend: Dynamic allocation enabled without spark.executor.cores explicitly set, you may get more executors allocated than expected. It's recommended to set spark.executor.cores explicitly. Please check SPARK-30299 for more details.\n",
      "24/03/10 22:13:09 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"
     ]
    }
   ],
   "source": [
    "#number of spark workers\n",
    "n_workers = \"2\"\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.145:7077\") \\\n",
    "        .appName(\"\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\", n_workers)\\\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", n_workers)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# create spark context and set log level to warn\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc14202-474a-4a1e-8972-db1b656880cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a score based on the difference from the \"perfect\" bpm\n",
    "def get_bpm_range(bpm_diff):\n",
    "    if bpm_diff < 10:\n",
    "        return 0.6\n",
    "    elif bpm_diff < 20:\n",
    "        return 0.4\n",
    "    elif bpm_diff < 30:\n",
    "        return 0.3\n",
    "    elif bpm_diff < 40:\n",
    "        return 0.1\n",
    "    else:\n",
    "        return 0.0\n",
    "#returns a score based on the difference from the \"perfect\" loudness\n",
    "\n",
    "def get_loudness_range(loudness_diff):\n",
    "    if loudness_diff < 2:\n",
    "        return 0.4\n",
    "    elif loudness_diff < 15:\n",
    "        return 0.3\n",
    "    elif loudness_diff < 35:\n",
    "        return 0.2\n",
    "    elif loudness_diff < 40:\n",
    "        return 0.1\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "get_bpm_range_udf = udf(get_bpm_range, FloatType())\n",
    "get_loudness_range_udf = udf(get_loudness_range, FloatType())\n",
    "\n",
    "#calculates the danceability based on loudness and bpm cols in a spark dataframe\n",
    "def calculate_danceability(loudness_col, bpm_col):\n",
    "    # Calculate differences\n",
    "    loudness_diff = abs(loudness_col + 7)\n",
    "    bpm_diff = abs(bpm_col - 120)\n",
    "\n",
    "    # Get the scores \n",
    "    loudness_score = get_loudness_range_udf(loudness_diff)\n",
    "    bpm_score = get_bpm_range_udf(bpm_diff)\n",
    "\n",
    "    # Calculate danceability\n",
    "    danceability = bpm_score + loudness_score\n",
    "    \n",
    "    return danceability\n",
    "\n",
    "#year to decade function, year must be an int\n",
    "def year_to_decade_udf(year):\n",
    "    return year - (year % 10)\n",
    "\n",
    "year_to_decade_spark_udf = udf(year_to_decade_udf, IntegerType())\n",
    "\n",
    "#pruning function for spark dataframe\n",
    "def prune_data(df):\n",
    "    # Remove rows with missing values\n",
    "    df = df.dropna()\n",
    "    # Remove rows with invalid values\n",
    "    df = df.filter((col('year') > 0) &\n",
    "                   (col('bpm') > 0) & (col('bpm') < 300) &\n",
    "                   (col('loudness') > -100) & (col('loudness') < 0))\n",
    "    return df\n",
    "\n",
    "#preprocess data, drop unneccesary columns, calculate danceablity and change years to decae\n",
    "def preprocess_data(df):\n",
    "    # Calculate danceability\n",
    "    df = df.withColumn('year', col('year').cast(IntegerType()))\n",
    "    df = df.withColumn('loudness', col('loudness').cast(IntegerType()))\n",
    "    df = df.withColumn('bpm', col('bpm').cast(IntegerType()))\n",
    "    pruned_df = prune_data(df)\n",
    "    #calculate the danceability and round it\n",
    "    pruned_df = pruned_df.withColumn('danceability', calculate_danceability(col('loudness'), col('bpm')))\n",
    "    pruned_df = pruned_df.withColumn('danceability', round(col('danceability'), 2))\n",
    "    #change the years into decades\n",
    "    pruned_df = pruned_df.withColumn('decade', year_to_decade_spark_udf(col('year')))\n",
    "    pruned_df = pruned_df.drop('loudness', 'bpm', 'year')\n",
    "    return pruned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e59c757-0b39-4dc4-b315-033e570b4f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows in original df: 10000000\n",
      "Number of Columns in original df: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 4) / 4]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoopuser/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/hadoopuser/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m preprocessed_df \u001b[38;5;241m=\u001b[39m preprocess_data(spark_df)\n\u001b[1;32m     27\u001b[0m num_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(preprocessed_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m---> 28\u001b[0m num_rows \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Rows after preprocessing:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_rows)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Columns after preprocessing:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_columns)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#list to save the times for each run and number of runs\n",
    "times = []\n",
    "num_runs = 3\n",
    "print('running :)')\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    #get start time\n",
    "    start = time.time()\n",
    "    \n",
    "    file_path = 'hdfs://192.168.2.145:5000/de11project/aggregated_song_features_2000x.csv' \n",
    "    \n",
    "    # Read the CSV file into a spark dataframe\n",
    "    spark_df = spark_session.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .load(file_path)\n",
    " \n",
    "    # printing number of rows and cols \n",
    "    num_rows = spark_df.count()\n",
    "    num_columns = len(spark_df.columns)\n",
    "    \n",
    "    print(\"Number of Rows in original df:\", num_rows)\n",
    "    print(\"Number of Columns in original df:\", num_columns)\n",
    "\n",
    "    #preprocess data and counting number of rows and cols\n",
    "    preprocessed_df = preprocess_data(spark_df)\n",
    "    \n",
    "    num_columns = len(preprocessed_df.columns)\n",
    "    num_rows = preprocessed_df.count()\n",
    "    \n",
    "    print(\"Number of Rows after preprocessing:\", num_rows)\n",
    "    print(\"Number of Columns after preprocessing:\", num_columns)\n",
    "\n",
    "    #show the first 10 rows of preprocessed data\n",
    "    preprocessed_df.show(10)\n",
    "\n",
    "    #get mean and median values grouped by decade\n",
    "    grouped_decade = preprocessed_df.groupBy('decade')\\\n",
    "    .agg(mean('danceability').alias('mean_danceability'), \n",
    "         expr('percentile_approx(danceability, 0.5)').alias('median_danceability'), \n",
    "         count('*').alias('count'))\\\n",
    "    .withColumn('mean_danceability', round('mean_danceability', 2))\\\n",
    "    .withColumn('median_danceability', round('median_danceability', 2))\\\n",
    "    .select('decade', 'mean_danceability', 'median_danceability', 'count')\n",
    "    \n",
    "    # Show the first 10 rows of the grouped data\n",
    "    grouped_decade.show(10)\n",
    "    \n",
    "    #collect decades for plotting\n",
    "    decades = grouped_decade.select('decade').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    #collect mean danceablity to plot\n",
    "    mean_danceability = grouped_decade.select('mean_danceability').rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Plotting mean graph\n",
    "    plt.bar(decades, mean_danceability, color='#ffdfba', width=2)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Mean Danceability')\n",
    "    plt.xlabel('Decade')\n",
    "    plt.title('Mean Danceability by Decade')\n",
    "    plt.savefig('../images/mean_danceability_by_decade.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    #collect median danceablity to plot\n",
    "    median_danceability = grouped_decade.select('median_danceability').rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Plotting median graph\n",
    "    plt.bar(decades, median_danceability, color='#ffb3ba', width=2)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Median Danceability')\n",
    "    plt.xlabel('Decade')\n",
    "    plt.title('Median Danceability by Decade')\n",
    "    plt.savefig('../images/median_danceability_by_decade.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Group by rounded danceability scores and count the number of songs for each score\n",
    "    danceability_score_count = preprocessed_df.groupBy('danceability').count().orderBy('danceability')\n",
    "\n",
    "    # Show the result\n",
    "    danceability_score_count.show()\n",
    "    \n",
    "    # Collect data to plot\n",
    "    scores = danceability_score_count.select('danceability').rdd.flatMap(lambda x: x).collect()\n",
    "    counts = danceability_score_count.select('count').rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Plotting\n",
    "    plt.bar(scores, counts, color='#bae1ff', width=0.02)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Danceability Score')\n",
    "    plt.title('Count of Songs by Danceability')\n",
    "    plt.savefig('../images/count_of_songs_by_danceability.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    #get end time\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "\n",
    "print(f\"Times with {n_workers} workers:\")\n",
    "print(f\"For file:{file_path}\")\n",
    "print(times)\n",
    "print(f\"Average time: {np.mean(times)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9968a3d-2c2b-4d55-b53f-b521814972d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopping the spark session\n",
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
